
* Imperative programming uses a thread per request(Tomcat).
* Reactive programming uses a loop of events(Netty) with no blocking threads.

* Traditional JDBC uses blocking threads with HikariCP
* R2DBC uses a Reactive Connection Pool that keeps open connections (async)


01-process-transaction
02-


***********************************************************************

"Real-Time Fraud Alert System"

Componentes Técnicos de la Solución

1. Ingesta: Transaction-Producer (WebFlux + Kafka)
    En lugar de un controlador REST tradicional, este servicio manejará las transacciones entrantes de forma asíncrona.
    Tecnología: Spring WebFlux + Reactor Kafka.
    Clave: El uso de KafkaSender de Project Reactor permite que el envío del mensaje sea una operación no bloqueante que devuelve un Mono<SenderResult<Void>>.

2. El Cerebro: Fraud-Processor (Stream Processing)
    Este es el corazón reactivo. Consumirá el flujo de Kafka como un Flux<Transaction>.
    Estrategia de Ventana (Windowing): Podrías usar operadores como window() o buffer() para analizar, por ejemplo, si un usuario ha hecho más de 5 transacciones en los últimos 30 segundos (detección de patrones).
    Backpressure: Si Kafka envía datos más rápido de lo que tu lógica de fraude puede procesar, el flujo reactivo gestionará la demanda para no desbordar la memoria (Heap).

3. Persistencia: R2DBC y Redis Reactivo
    Aquí es donde muchos fallan. Si usas Hibernate tradicional (JDBC), bloqueas los hilos de Netty.
    R2DBC (Reactive Relational Database Connectivity): Para persistir las transacciones sospechosas en PostgreSQL sin bloquear.
    Spring Data Reactive Redis: Para consultar en milisegundos las "listas negras" o límites de crédito.

4. Observabilidad (Tu especialidad)
    Dado que en el mundo reactivo el "Stack Trace" tradicional se vuelve confuso (porque los hilos saltan de una tarea a otra), la arquitectura incluirá:
    Micrometer Tracing: Para propagar el traceId a través de los flujos asíncronos.
    Kibana/Dynatrace: Para monitorear el rendimiento de los Event Loops.

***********************************************************************

Levantar base de datos postgres:

docker-compose up -d

Para testear:
docker exec -it reactive-postgres-banking psql -U user_banking -d fraud_db -c "SELECT customer_id, daily_max_amount FROM customer_limits;"
docker exec -it reactive-postgres-banking psql -U user_banking -d fraud_db -c "SELECT count(*) FROM transactions;"

Probar fraud-detection-service:

curl -X POST http://localhost:8081/api/v1/transactions \
-H "Content-Type: application/json" \
-d '{
    "transactionId": "TXN-2026-WEB-001",
    "customerId": "CUST-001",
    "accountId": "ACC-101",
    "amount": 500.00,
    "currency": "PEN",
    "operationType": "DEBIT",
    "channel": "WEB"
}'

docker exec -it reactive-postgres-banking psql -U user_banking -d fraud_db -c "SELECT * FROM customer_limits WHERE customer_id = 'CUST-001';"

       
- rm -rf /tmp/kafka-logs /tmp/zookeeper /tmp/kraft-combined-logs
- KAFKA_CLUSTER_ID="$(bin/kafka-storage.sh random-uuid)"
- echo $KAFKA_CLUSTER_ID
- bin/kafka-storage.sh format -t $KAFKA_CLUSTER_ID -c config/kraft/server.properties
- bin/kafka-server-start.sh config/kraft/server.properties

- bin/kafka-topics.sh --bootstrap-server localhost:9092 --list
- bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic fraud-detection-events --from-beginning

bin/kafka-console-consumer.sh --bootstrap-server 127.0.0.1:9092 \
--topic fraud-detection-events \
--from-beginning \
--formatter kafka.tools.DefaultMessageFormatter \
--property print.key=true \
--property print.value=true \
--property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer \
--property value.deserializer=org.apache.kafka.common.serialization.StringDeserializer


- bin/kafka-server-stop.sh

- to configure application.properties, for the wsl use: ip addr show, use the eth0  → spring.kafka.bootstrap-servers=172.30.84.55:9092
- edit server.properties with this:
    listeners=PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093
    advertised.listeners=PLAINTEXT://172.30.84.55:9092






***********************************************************************

Datos nuevos e importantes:

Reactive Streams: estándar, conjunto de 4 interfaces: Publisher, Subscriber, Subscription y Processor) que definen cómo deben fluir los datos de forma asíncrona y con Backpressure (capacidad del consumidor para avisar que está saturado).

Project Reactor,: Librería de Tipos,Flujos de datos (Mono/Flux)
Spring WebFlux: Framework Web, Peticiones HTTP, Rutas, API REST
RxJava: Librería de Tipos, Flujos de datos (Observable)

* ISO Codes: Usar mcc (Merchant Category Code) es vital; si un cliente de Lima compra en una joyería en Rusia 5 minutos después de comprar un café en San Isidro, tu lógica reactiva detectará el fraude de inmediato.
* Índices para no hacer full table scan sino ir a los registros adecuados en millis
* Configurar healt-check(docker-compose) para evitar caídas de app si la bd no está disponible o aún está levantando
* Non-blocking I/O: Al devolver Mono<TransactionEntity> en el Controller, el hilo de Netty no se queda esperando a que la base de datos responda. Se libera inmediatamente para atender otras peticiones y vuelve cuando los datos están listos.
* MDC (Mapped Diagnostic Context) - correlation id es necesario configurarlo porque se pierde en el contexto reactivo. Lo hice con deferContextual(se tendría que implementar en cada método). Y también con Micrometer.
* Persistable en el Entity para que Spring sepa si es un INSERT o UPDATE
* Mono.defer() En programación reactiva, si no usas defer, el código dentro del switchIfEmpty se prepararía (instanciaría) incluso si la transacción ya existe. defer asegura que la lógica de negocio solo se ejecute si realmente la transacción es nueva.
* http://localhost:8081/swagger-ui.html
* map: para conversiones simple sincronas | flatmap: para seguir realizando operaciones async


***********************************************************************

Operadores

1. map
Se usa para transformar los elementos de un flujo uno a uno. Toma un valor, le aplica una función y devuelve el nuevo valor.

Analogía: Tienes una lista de precios en soles y los multiplicas todos por el tipo de cambio para obtenerlos en dólares.

Ejemplo:
Flux<String> nombres = Flux.just("julio", "chacon");
Flux<String> enMayusculas = nombres.map(n -> n.toUpperCase());
// Resultado: "JULIO", "CHACON"

------

2. flatMap
A diferencia de map, flatMap se usa cuando la transformación devuelve otro flujo (un Mono o Flux) en lugar de un valor simple. Es ideal para llamadas asíncronas encadenadas (como ir a la DB tras recibir un ID).

Dato clave: No garantiza el orden original de los elementos, ya que procesa las suscripciones en paralelo.

Ejemplo:
Flux<String> ids = Flux.just("ID-1", "ID-2");
Flux<User> usuarios = ids.flatMap(id -> service.findUserById(id)); 
// findUserById devuelve un Mono<User>, flatMap lo "aplana" para que no sea Flux<Mono<User>>

------

3. switchIfEmpty
Es el "Plan B". Si el flujo original llega al final sin haber emitido ningún dato (está vacío), este operador salta y ejecuta un flujo alternativo.

Uso común: Buscar en caché; si no hay nada, buscar en la base de datos de Oracle.

Ejemplo:
Mono<String> cache = Mono.empty();
Mono<String> resultado = cache.switchIfEmpty(Mono.just("Dato de la DB"));
// Resultado: "Dato de la DB"

-------

4. zip
Se utiliza para combinar dos o más flujos. Espera a que todos los flujos tengan un elemento disponible y luego los une en un nuevo objeto (como una Tuple o un objeto personalizado).

Analogía: Como una cremallera (zipper); necesitas ambos lados para avanzar.

Ejemplo:
Mono<String> nombre = Mono.just("Julio");
Mono<String> cargo = Mono.just("Backend Dev");

Mono<String> combinado = Mono.zip(nombre, cargo)
    .map(tuple -> tuple.getT1() + " es " + tuple.getT2());
// Resultado: "Julio es Backend Dev"