
* Imperative programming uses a thread per request(Tomcat).
* Reactive programming uses a loop of events(Netty) with no blocking threads.

* Traditional JDBC uses blocking threads with HikariCP
* R2DBC uses a Reactive Connection Pool that keeps open connections (async)


01-process-transaction
02-notification-service
03-oauth2-jwt
04-observability-context-propagation
05-kafka-retries-backoff
06-transactional-outbox-pattern
07-docker-kubernetes
08-helm-infrastructure


***********************************************************************

"Real-Time Fraud Alert System"

Componentes Técnicos de la Solución

1. Ingesta: Transaction-Producer (WebFlux + Kafka)
    En lugar de un controlador REST tradicional, este servicio manejará las transacciones entrantes de forma asíncrona.
    Tecnología: Spring WebFlux + Reactor Kafka.
    Clave: El uso de KafkaSender de Project Reactor permite que el envío del mensaje sea una operación no bloqueante que devuelve un Mono<SenderResult<Void>>.

2. El Cerebro: Fraud-Processor (Stream Processing)
    Este es el corazón reactivo. Consumirá el flujo de Kafka como un Flux<Transaction>.
    Estrategia de Ventana (Windowing): Podrías usar operadores como window() o buffer() para analizar, por ejemplo, si un usuario ha hecho más de 5 transacciones en los últimos 30 segundos (detección de patrones).
    Backpressure: Si Kafka envía datos más rápido de lo que tu lógica de fraude puede procesar, el flujo reactivo gestionará la demanda para no desbordar la memoria (Heap).

3. Persistencia: R2DBC y Redis Reactivo
    Aquí es donde muchos fallan. Si usas Hibernate tradicional (JDBC), bloqueas los hilos de Netty.
    R2DBC (Reactive Relational Database Connectivity): Para persistir las transacciones sospechosas en PostgreSQL sin bloquear.
    Spring Data Reactive Redis: Para consultar en milisegundos las "listas negras" o límites de crédito.

4. Observabilidad (Tu especialidad)
    Dado que en el mundo reactivo el "Stack Trace" tradicional se vuelve confuso (porque los hilos saltan de una tarea a otra), la arquitectura incluirá:
    Micrometer Tracing: Para propagar el traceId a través de los flujos asíncronos.
    Kibana/Dynatrace: Para monitorear el rendimiento de los Event Loops.

***********************************************************************

Levantar base de datos postgres:

sudo rm -rf postgres-reactive   ----> borrar carpeta con contenido
sudo mkdir postgres-reactive

sudo chown -R jchaconv:jchaconv /home/jchaconv/postgres-reactive   ---> dar permisos sudo
ls -ld postgres-reactive    --> ejecutar desde aquí /home/jchaconv/ para validar permisos

docker-compose up -d

Para testear:
docker exec -it reactive-postgres-banking psql -U user_banking -d fraud_db -c "SELECT customer_id, daily_max_amount FROM customer_limits;"
docker exec -it reactive-postgres-banking psql -U user_banking -d fraud_db -c "SELECT count(*) FROM transactions;"
docker exec -it reactive-postgres-banking psql -U user_banking -d fraud_db -c "SELECT * FROM transactions;"
docker exec -it reactive-postgres-banking psql -U user_banking -d fraud_db -c "SELECT * FROM transactions WHERE customer_id = 'CUST-001';"


Probar fraud-detection-service:

curl -X POST http://localhost:8081/api/v1/transactions \
-H "Content-Type: application/json" \
-d '{
    "transactionId": "TXN-2026-WEB-001",
    "customerId": "CUST-001",
    "accountId": "ACC-101",
    "amount": 500.00,
    "currency": "PEN",
    "operationType": "DEBIT",
    "channel": "WEB"
}'

docker exec -it reactive-postgres-banking psql -U user_banking -d fraud_db -c "SELECT * FROM customer_limits WHERE customer_id = 'CUST-001';"

docker exec -it reactive-postgres-banking psql -U user_banking -d fraud_db -c "SELECT * FROM outbox_events;"


-----------------------------------------

Configuración Kafka server:
       
- rm -rf /tmp/kafka-logs /tmp/zookeeper /tmp/kraft-combined-logs
- KAFKA_CLUSTER_ID="$(bin/kafka-storage.sh random-uuid)"
- echo $KAFKA_CLUSTER_ID
- bin/kafka-storage.sh format -t $KAFKA_CLUSTER_ID -c config/kraft/server.properties
- bin/kafka-server-start.sh config/kraft/server.properties

- bin/kafka-topics.sh --bootstrap-server localhost:9092 --list
- bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic fraud-detection-events --from-beginning

bin/kafka-console-consumer.sh --bootstrap-server 127.0.0.1:9092 \
--topic fraud-detection-events \
--from-beginning \
--formatter kafka.tools.DefaultMessageFormatter \
--property print.key=true \
--property print.value=true \
--property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer \
--property value.deserializer=org.apache.kafka.common.serialization.StringDeserializer


- bin/kafka-topics.sh --bootstrap-server localhost:9092 --delete --topic fraud-detection-events

- bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list

- bin/kafka-server-stop.sh

- to configure application.properties, for the wsl use: ip addr show, use the eth0  → spring.kafka.bootstrap-servers=172.30.84.55:9092
- edit server.properties with this:
    listeners=PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093
    advertised.listeners=PLAINTEXT://172.30.84.55:9092

- bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic fraud-detection-events-dlt --from-beginning

bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 \
--topic fraud-detection-events-dlt \
--from-beginning \
--property print.headers=true


-----------------------------------------

Levantar Redis:

docker run -d --name redis-banking -p 6379:6379 redis:latest

# Entrar al CLI del contenedor
docker exec -it redis-banking redis-cli
# 1. Listar todas las llaves (Cuidado en producción, es lento)
keys *
# 2. Buscar solo las de idempotencia
keys idempotency:txn:*
# 3. Ver el valor de una transacción específica
get idempotency:txn:TXN-ERROR-002
# 4. Ver cuánto tiempo de vida le queda (en segundos)
ttl idempotency:txn:TXN-ERROR-002

FLUSHALL   ---> para borrar todo

***********************************************************************

Para levantar todo el ecosistema:

sudo mkdir fraud-alert-system
sudo chown -R jchaconv:jchaconv /home/jchaconv/fraud-alert-system   ---> dar permisos sudo
ls -ld fraud-alert-system    --> ejecutar desde aquí /home/jchaconv/ para validar permisos

tener esta estructura:

fraud-alert-system/
├── auth-server/             (Carpeta del microservicio)
├── fraud-detection-service/ (Carpeta del microservicio)
├── notification-service/    (Carpeta del microservicio)
├── scripts/                 <-- AQUÍ pones el script
│   └── init.sql
└── docker-compose.yaml      <-- El YAML en la raíz

chmod 644 /home/jchaconv/fraud-alert-system/scripts/init.sql   ---> Permisos de ejecución: Asegúrate de que el archivo tenga permisos de lectura dentro de WSL
chmod +x auth-server/mvnw
chmod +x fraud-detection-service/mvnw
chmod +x notification-service/mvnw

docker-compose up --build -d     -----> cuando necesito compilar de nuevo las imágenes
docker-compose ps   --> verificar estados


Solo la primera vez: Recuerda que Postgres solo ejecuta los scripts en /docker-entrypoint-initdb.d/ la primera vez que se crea el volumen.
Si haces cambios en el init.sql y quieres que se reflejen, debes borrar el contenedor y su volumen:

docker-compose down   ----> para limpiar los containers
docker-compose up -d     ----> cuando solo hay cambios en el docker-compose.yaml

docker logs -f fraud-detection-service
docker logs -f notification-service
docker logs -f auth-server


docker exec kafka-banking /opt/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list
docker exec -it reactive-postgres-banking psql -U user_banking -d fraud_db
SELECT * FROM transactions WHERE customer_id = 'CUST-001';

docker exec -it redis-banking redis-cli

curl http://localhost:8081/actuator/health

poner en hosts: 127.0.0.1 auth-server para que se genere la firma con ese host y no con uno no autorizado como "localhost"

test outbox:

docker stop kafka-banking

docker exec -it reactive-postgres-banking psql -U user_banking -d fraud_db -c "SELECT * FROM transactions WHERE customer_id = 'CUST-001';"
docker exec -it reactive-postgres-banking psql -U user_banking -d fraud_db -c "SELECT * FROM outbox_events;"

docker start kafka-banking

---------------------------------------------------------

Kubernetes Config:

---------------------
Install kubectl:
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
chmod +x kubectl
sudo mv kubectl /usr/local/bin/

# Verifica con:
kubectl version --client
---------------------
Install Kind:

curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.22.0/kind-linux-amd64
chmod +x ./kind
sudo mv ./kind /usr/local/bin/kind
---------------------
Crear primer cluster:
kind create cluster --name banking-cluster

Validar acceso:
kubectl cluster-info
kubectl get nodes
---------------------
Crear namespace:
kubectl create namespace banking-ns
---------------------
Crear un ConfigMap -> usar banking-config.yaml
kubectl apply -f banking-config.yaml
---------------------
Para configurar Postgres
Persistent Volume Claim -> postgres-pvc.yaml -> le dice a K8S "Resérvame 1GB de disco que sobreviva a reinicios".
También se usará el archivo postgres-db.yaml

# 1. Aplicar los archivos
kubectl apply -f postgres-pvc.yaml
kubectl apply -f postgres-db.yaml

# 2. Verificar que el Pod esté corriendo
kubectl get pods -n banking-ns

# 3. Verificar el Service (Este es el que usará tu Java app)
kubectl get svc -n banking-ns

* Self-Healing:
Si ejecuto esto:
kubectl delete pod <nombre-del-pod-postgres> -n banking-ns
K8s creará uno nuevo automáticamente

---------------------

Construir imágenes para subir a Docker Hub

Configurar PAT en Docker Hub -> settings

docker login -u jchaconv   -> luego pedirá el token
docker logout

# 1. Construir la imagen (asegúrate de estar en la carpeta del proyecto)
docker build -t jchaconv/fraud-detection-service:latest .

# 2. Subirla a la nube (Push)
docker push jchaconv/fraud-detection-service:latest

Ver en la parte de repositories de Docker Hub la imagen subida.
---------------------
Config de Redis:
usar redis-k8s.yaml
kubectl apply -f redis-k8s.yaml
---------------------
Crear fraud-service-deployment.yaml
kubectl apply -f fraud-service-deployment.yaml
Verificar el estado:
# Ver si el pod está en Running o da error
kubectl get pods -n banking-ns

# Ver los logs de tu Spring Boot para confirmar conexión a DB y Redis
kubectl logs -f deployment/fraud-detection-app -n banking-ns

Como estoy usando wsl sale error para descargar de internet. Vamos a inyectar directamente
desde mi máquina al cluster:
# 1. Cargar Postgres
docker pull postgres:15-alpine
kind load docker-image postgres:15-alpine --name banking-cluster

# 2. Cargar Redis
docker pull redis:7-alpine
kind load docker-image redis:7-alpine --name banking-cluster

# 3. Cargar tu App
kind load docker-image jchaconv/fraud-detection-service:latest --name banking-cluster

esta línea debe estar en los archivos(solo los tres de abajo) yaml: imagePullPolicy: IfNotPresent

Reiniciar despliegues:
kubectl apply -f postgres-db.yaml
kubectl apply -f redis-k8s.yaml
kubectl apply -f fraud-service-deployment.yaml

Verificar:
kubectl get pods -n banking-ns

Meter bien el init.sql para la base de datos:
kubectl create configmap postgres-init-script --from-file=init.sql -n banking-ns

actualizar el archivo de postgres (ya lo tengo actualizado)

# 1. Borramos el despliegue actual
kubectl delete deployment postgres-db -n banking-ns

# 2. Borramos el PVC para limpiar los datos previos (OJO: Esto borra tus datos actuales)
kubectl delete pvc postgres-pvc -n banking-ns

# 3. Aplicamos todo de nuevo
kubectl apply -f postgres-pvc.yaml
kubectl apply -f postgres-db.yaml

kubectl logs -f deployment/postgres-db -n banking-ns
---------------------
Probar App:
Hacer tunnel primero
kubectl port-forward svc/fraud-detection-entrypoint 8081:8081 -n banking-ns
http://localhost:8081/actuator/health
---------------------
Config Kafka -> usar kafka-k8s.yaml
docker pull apache/kafka:3.7.0
kind load docker-image apache/kafka:3.7.0 --name banking-cluster
kubectl apply -f kafka-k8s.yaml
kubectl logs -f deployment/kafka-db -n banking-ns
---------------------
Auth-Server:
# Construir la imagen
docker build -t jchaconv/auth-server:latest .

# Cargarla a Kind para evitar el error de descarga en WSL
kind load docker-image jchaconv/auth-server:latest --name banking-cluster

kubectl apply -f auth-server-k8s.yaml

Tunnel para Postman:
Para usar exactamente la misma URL localhost:9000 desde Windows:
kubectl port-forward svc/auth-service 9000:9000 -n banking-ns

kubectl logs -f deployment/auth-server -n banking-ns

---------------------
notification-service:

# 1. Construir la imagen
docker build -t jchaconv/notification-service:latest .

# 2. Inyectarla a Kind
kind load docker-image jchaconv/notification-service:latest --name banking-cluster

kubectl apply -f notification-service-k8s.yaml

---------------------

Probar:
kubectl logs -f deployment/fraud-detection-app -n banking-ns
kubectl logs -f deployment/notification-service -n banking-ns

# Entrar directo al prompt de SQL
kubectl exec -it postgres-db-5b9f955d7-czvjd -n banking-ns -- psql -U jchacon -d fraud_db
\dt
SELECT * FROM customer_limits;
SELECT * FROM outbox_events;
SELECT * FROM transactions;
\q

# Entrar directo al cliente de Redis
kubectl exec -it redis-db-7bf66994d7-cx4pf -n banking-ns -- redis-cli

KEYS *
GET <key_name>
FLUSHALL
exit


Si quiero usar DBeaver, necesito tunnel:
kubectl port-forward svc/postgres-service 5432:5432 -n banking-ns


Validar si el pod puede ver a otro:
kubectl exec -it fraud-detection-app-6d7d5f6758-mrghd -n banking-ns -- curl -v http://auth-service:9000/.well-known/openid-configuration


Si se hace un nuevo deploy, para asegurar que tome las nuevas variables del archivo .yaml:
kubectl rollout restart deployment/fraud-detection-app -n banking-ns


# Ejecuta este comando para activar logs detallados de seguridad
kubectl set env deployment/fraud-detection-app \
  logging.level.org.springframework.security=DEBUG \
  logging.level.org.springframework.security.oauth2=DEBUG \
  -n banking-ns


# Para probar con Kafka caído:
kubectl scale deployment kafka-db --replicas=0 -n banking-ns
kubectl get pods -n banking-ns

kubectl scale deployment kafka-db --replicas=1 -n banking-ns

Pruebas hasta aquí ok, happy path y error con Kafka caído.

***********************************************************************

HELM:

* Install helm:
    curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
    helm version

* Activar autocompletado:
    echo "source <(helm completion bash)" >> ~/.bashrc
    source ~/.bashrc

* Crear esta estructura de carpetas:

    banking-system-chart/
    ├── Chart.yaml
    ├── values.yaml
    └── templates/
        └── fraud-service.yaml

* Dry Run - sirve para DEBUG (simula el despliegue sin aplicar nada, para ver qué YAML generaría):
    helm install banking-app ./banking-system-chart --dry-run --debug

* Validar chart:
    helm lint ./banking-system-chart

* Crear el cluster de K8s:
    kind create cluster --name banking-cluster
    kubectl create namespace banking-ns

* Instalar:
    helm install banking-app ./banking-system-chart -n banking-ns

    Si se requiere borrar lo instalado:
    helm uninstall banking-app -n default
    kubectl delete all --all -n banking-ns

* Error de red:
    kind load docker-image jchaconv/fraud-detection-service:latest --name banking-cluster
    kind load docker-image jchaconv/auth-server:latest --name banking-cluster
    kind load docker-image jchaconv/notification-service:latest --name banking-cluster    
    kind load docker-image apache/kafka:3.7.0 --name banking-cluster
    kind load docker-image postgres:15-alpine --name banking-cluster
    kind load docker-image redis:7-alpine --name banking-cluster

* Validar recursos:
    kubectl get all -n banking-ns

* Post-instalación:
    kubectl get pods -n banking-ns -w
    helm list -n banking-ns
    kubectl logs deployment/postgres-db -n banking-ns

* Hacer tunnel al service:
    kubectl port-forward svc/fraud-detection-app-entrypoint 8081:8081 -n banking-ns
    kubectl port-forward svc/auth-service 9000:9000 -n banking-ns

* Ejecutar SQL y ver logs:    
    - postgresql:
        kubectl exec -it deployment/postgres-db -n banking-ns -- psql -U jchacon -d fraud_db
            \dt
            SELECT * FROM customer_limits;
            SELECT * FROM outbox_events;
            \q

    - redis:
        kubectl exec -it deployment/redis-db -n banking-ns -- redis-cli

    - fraud-service
        kubectl logs -f deployment/fraud-detection-app -n banking-ns
    
    - notification-service
        kubectl logs -f deployment/notification-service -n banking-ns

    - Si se reinició el pod y se quiere ver error anterior:
        kubectl logs deployment/fraud-detection-app -n banking-ns --previous

* Probar happy path:ok, para probar con Kafka caído:ok

    kubectl scale deployment kafka-db --replicas=0 -n banking-ns
    kubectl get pods -n banking-ns
    kubectl scale deployment kafka-db --replicas=1 -n banking-ns

* Para borrar solo la aplicación manteniendo el cluster y la infraestructura base
    helm uninstall banking-app -n banking-ns

* Para borrar todo el cluster de Kind de raíz
    kind delete cluster --name banking-cluster

***********************************************************************


Datos nuevos e importantes:

Reactive Streams: estándar, conjunto de 4 interfaces: Publisher, Subscriber, Subscription y Processor) que definen cómo deben fluir los datos de forma asíncrona y con Backpressure (capacidad del consumidor para avisar que está saturado).

Project Reactor,: Librería de Tipos,Flujos de datos (Mono/Flux)
Spring WebFlux: Framework Web, Peticiones HTTP, Rutas, API REST
RxJava: Librería de Tipos, Flujos de datos (Observable)

* ISO Codes: Usar mcc (Merchant Category Code) es vital; si un cliente de Lima compra en una joyería en Rusia 5 minutos después de comprar un café en San Isidro, tu lógica reactiva detectará el fraude de inmediato.
* Índices para no hacer full table scan sino ir a los registros adecuados en millis
* Configurar healt-check(docker-compose) para evitar caídas de app si la bd no está disponible o aún está levantando
* Non-blocking I/O: Al devolver Mono<TransactionEntity> en el Controller, el hilo de Netty no se queda esperando a que la base de datos responda. Se libera inmediatamente para atender otras peticiones y vuelve cuando los datos están listos.
* MDC (Mapped Diagnostic Context) - correlation id es necesario configurarlo porque se pierde en el contexto reactivo. Lo hice con deferContextual(se tendría que implementar en cada método). Y también con Micrometer.
* Persistable en el Entity para que Spring sepa si es un INSERT o UPDATE
* Mono.defer() En programación reactiva, si no usas defer, el código dentro del switchIfEmpty se prepararía (instanciaría) incluso si la transacción ya existe. defer asegura que la lógica de negocio solo se ejecute si realmente la transacción es nueva.
* http://localhost:8081/swagger-ui.html
* map: para conversiones simple sincronas | flatmap: para seguir realizando operaciones async
* El bean authorizationServerSecurityFilterChain en auth-server activa por defecto la url que da el token
* Micrometer Tracing basado en OpenTelemetry/Brave. Observabilidad, propagation context.
* traceId Es único para toda la transacción distribuida. Si la petición pasa por 5 microservicios, todos compartirán el mismo Trace ID. Sirve para correlacionar logs de distintos servicios.
* spanId Es único para cada bloque de trabajo (unidad lógica). 
* backoff: Es la estrategia de esperar un tiempo antes de reintentar una operación que falló.
* backpressure: Es un mecanismo donde el Consumidor le dice al Productor: "¡Oye, vas muy rápido, no puedo procesar tanto!". En WebFlux: Si tu base de datos es lenta y están llegando miles de peticiones HTTP,
  el driver R2DBC aplica backpressure hacia arriba para que Netty deje de aceptar nuevos bytes hasta que haya espacio en memoria.
* circuit breaker: Es un patrón que evita que una aplicación intente realizar una operación que probablemente va a fallar. Tiene 3 estados:
    - Closed (Cerrado): Todo bien, el tráfico fluye.
    - Open (Abierto): Hubo demasiados errores. El circuito se "rompe" y falla inmediatamente sin siquiera intentar llamar al servicio (protege los recursos).
    - Half-Open: Después de un tiempo, deja pasar unas pocas peticiones para ver si el servicio ya se recuperó.

* Transactional Outbox Pattern -> Outbox-First
* El Scheduler es el motor que garantiza que la "consistencia eventual" se cumpla: si el mensaje está en la tabla, tarde o temprano llegará a Kafka.

* Kind (Kubernetes in Docker): es una herramienta que crea clusters de Kubernetes usando contenedores de Docker como "nodos". Es lo que está permitiendo que tu cluster "banking-ns" viva dentro de tu máquina. Es ideal para desarrollo porque es ligero y se destruye/crea rápido.

* Kubernetes:
    - Un Pod es la unidad mínima. Dentro del pod van los containers, normalmente va solo uno. El Pod le da una IP.
    - Deployment, "estado deseado", "quiero siempre 3 réplicas de ese pod corriendo". Funciona como autoscaling de AWS.
    - Services, "load balancer interno" es una ip estática y nombre DNS parar apuntar a los PODs. Balancea la carga.
    - ConfigMap / Secret, es como environment vars, para configurar el application.properties sin recompilar.
    
    - Flujo de trabajo:
        - kubectl apply -f deployment.yaml   ---> k8s lee esto y hace que la realidad se vea así. Aquí se define la imagen docker a usar.
        - kubectl get pods   ---> veo el estado de mis containers.
        - kubectl logs <pod-name>
        - kubectl exec -it <pod-name> -- bash   ----> es como docker exec, para ver archivos o conectividad

    - Comandos para ver recursos básicos:
        - kubectl get ns
        - kubectl get pods -n banking-ns
        - kubectl get svc -n banking-ns
        - kubectl get deploy -n banking-ns
        - kubectl get cm -n banking-ns
        - kubectl get pvc -n banking-ns
        - kubectl get all -n banking-ns   ---> ver todo
        
        - Cuando hay problemas de conexión:
            - kubectl describe pod <nombre-del-pod> -n banking-ns
            - kubectl describe svc auth-service -n banking-ns

        - kubectl get pods --show-labels -n banking-ns  ---> validar services apuntando a pods correctos

        - kubectl get pods -n banking-ns -w  ---> monitorear en tiempo real

        - Para borrar
            - kubectl delete deployment auth-server -n banking-ns
            - kubectl delete service auth-service -n banking-ns

        - Borra todos los despliegues y servicios en el namespace
            - kubectl delete deployments,services --all -n banking-ns

        - Borrado total:
            - kubectl delete namespace banking-ns

        - Borrar y crear cluster:
            - kind delete cluster --name banking-cluster
            - kind create cluster --name banking-cluster

* ¿Qué es Helm y para qué sirve? Helm es el Package Manager de Kubernetes (el "Maven" o "NPM" de K8s).
    - Templatización: En lugar de tener valores fijos (como replicas: 1), usas variables (como replicas: {{ .Values.replicaCount }}).
    - Governance: Permite empaquetar todo tu sistema (Fraud + Auth + DBs) en una sola unidad llamada Chart.
    - Versioning: Si haces un despliegue y falla, puedes hacer un helm rollback y el sistema vuelve exactamente al estado anterior en segundos.

* YAMLs estáticos en un Banking-System Chart. Estructura:
    banking-chart/
        Chart.yaml          # Metadatos del proyecto
        values.yaml         # Aquí defines las variables (IPs, puertos, réplicas)
        templates/          # Tus YAMLs actuales pero con variables
            fraud-deployment.yaml
            auth-deployment.yaml
            services.yaml

* Despliegue atómico: En lugar de hacer kubectl apply a 5 archivos, harás:
helm upgrade --install banking-system ./banking-chart




***********************************************************************

Operadores

1. map
Se usa para transformar los elementos de un flujo uno a uno. Toma un valor, le aplica una función y devuelve el nuevo valor.

Analogía: Tienes una lista de precios en soles y los multiplicas todos por el tipo de cambio para obtenerlos en dólares.

Ejemplo:
Flux<String> nombres = Flux.just("julio", "chacon");
Flux<String> enMayusculas = nombres.map(n -> n.toUpperCase());
// Resultado: "JULIO", "CHACON"

------

2. flatMap
A diferencia de map, flatMap se usa cuando la transformación devuelve otro flujo (un Mono o Flux) en lugar de un valor simple. Es ideal para llamadas asíncronas encadenadas (como ir a la DB tras recibir un ID).

Dato clave: No garantiza el orden original de los elementos, ya que procesa las suscripciones en paralelo.

Ejemplo:
Flux<String> ids = Flux.just("ID-1", "ID-2");
Flux<User> usuarios = ids.flatMap(id -> service.findUserById(id)); 
// findUserById devuelve un Mono<User>, flatMap lo "aplana" para que no sea Flux<Mono<User>>

------

3. switchIfEmpty
Es el "Plan B". Si el flujo original llega al final sin haber emitido ningún dato (está vacío), este operador salta y ejecuta un flujo alternativo.

Uso común: Buscar en caché; si no hay nada, buscar en la base de datos de Oracle.

Ejemplo:
Mono<String> cache = Mono.empty();
Mono<String> resultado = cache.switchIfEmpty(Mono.just("Dato de la DB"));
// Resultado: "Dato de la DB"

-------

4. zip
Se utiliza para combinar dos o más flujos. Espera a que todos los flujos tengan un elemento disponible y luego los une en un nuevo objeto (como una Tuple o un objeto personalizado).

Analogía: Como una cremallera (zipper); necesitas ambos lados para avanzar.

Ejemplo:
Mono<String> nombre = Mono.just("Julio");
Mono<String> cargo = Mono.just("Backend Dev");

Mono<String> combinado = Mono.zip(nombre, cargo)
    .map(tuple -> tuple.getT1() + " es " + tuple.getT2());
// Resultado: "Julio es Backend Dev"